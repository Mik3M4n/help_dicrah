{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @iYume_ @OhmArsene @francoiscarpit1 @PorcherTh...\n",
       "1      L‚Äôobjectif de ce post √©tait simple : quelle la...\n",
       "2      Propos honteux et naus√©abonds... Quel est l'ob...\n",
       "3      Donc la y‚Äôa tellement 3 white qui veulent m‚Äôex...\n",
       "4      @YassEncore LAISSEZ LES ENFANTS CHOISIR DE LEU...\n",
       "5      Depuis qu'elle a r√©pondu √† une itw sur la s√©le...\n",
       "6      Tes une babtou tu peux pas comprendre https://...\n",
       "7      Ath√©e, je ne pourrais pas me sentir repr√©sent√©...\n",
       "8      Juste pour bien recontextualiser les choses : ...\n",
       "9      il ya un mec qui ma mentionn√© alors que  sa go...\n",
       "10     ¬´beurette¬ª c‚Äôest surtout un mot d√©gradant aux ...\n",
       "11     @Sakwws C'est pas mon probl√®me. Arr√™tez de ple...\n",
       "12     L'@UNEF d√©cide d'afficher le v√™tement le + rac...\n",
       "13     @CISWhiteMale4 @Arkhanciel @Nathandco @lordmah...\n",
       "14     Amazon fait venir des travailleurs √©trangers p...\n",
       "15     Si vous voyez quelqu‚Äôun avec ce type de lunett...\n",
       "16     C'est grave vrai, les mecs de la bac aussi ils...\n",
       "17     Vous imaginez une petite racaille de Daesh qui...\n",
       "18     @chxgtl Mais t‚Äôes stupide??? T‚Äôes blanche donc...\n",
       "19     Lallab condamne fermement ce d√©versement de ha...\n",
       "20     CP de UNEF Sorbonne Universit√©:\\n#SoutienAMary...\n",
       "21     #Wow puis on essaie de nous vendre #Trump comm...\n",
       "22     Bel article anticommuniste primaire qui n'aide...\n",
       "23     L‚Äô #UNEF s‚Äôoffusque de la d√©nonciation de sa d...\n",
       "24     @TrumpFrance Salut les losers,\\nLe terrorisme ...\n",
       "25     Les Italiens vont les prolonger dans une sauce...\n",
       "26     @VendeenOL @MassiliaFan26 @Twitter @TwitterFra...\n",
       "27     Aujourd‚Äôhui, il va sans doute encore y avoir d...\n",
       "28     Je me suis fais traiter de ¬´¬†babtou¬†fragile¬ª p...\n",
       "29     \"Non mais franchement ! Une adulte qui porte l...\n",
       "                             ...                        \n",
       "102    14 Mai\\n\\n#AurelieChatelainüá´üá∑\\n\\n1121 jours  q...\n",
       "103    Brand NEW üö® Retrouvez moi sur ce hit , ce clas...\n",
       "104    Avec tant d'autres,ils sont morts pour leur pa...\n",
       "105    En 2011, le NPA s'√©tait d√©chir√© sur le choix d...\n",
       "106    Donc questionner l‚Äôincoh√©rence id√©ologique d‚Äôu...\n",
       "107    @iSwiizYT @WEIZKOO @Kuzo_TR @Thenewpxx @TheAig...\n",
       "108    Arr√™tez de dire les mots \" beurette \", \" niafo...\n",
       "109    @RAMAHERVE @suavelos_fr Ce qui est p√©nible, c'...\n",
       "110    La belle h√¥tesse de l'air tunisienne Ghofrane ...\n",
       "111    La gauche immigrationniste le matin :  ouin ou...\n",
       "112    L'UNEF condamne un \"d√©ferlement de haine racis...\n",
       "113    @solere92 @gouvernementFR Vous n‚Äôavez pas hont...\n",
       "114    @Foreign_Cat @HuffPostQuebec Seulement s√©parat...\n",
       "115    @RachSago @MabsleTouns @BernardCalmels @Sara_S...\n",
       "116    O√π est pass√©e la gauche fran√ßaise qui a tant c...\n",
       "117    WMDKSKSKDKDKDK wallaye, √ßa va commenter tous s...\n",
       "118    M√©diapart et Soral, m√™me combat : \"ne pas aime...\n",
       "119    Telle est Rokhaya : un jour glamour et sourian...\n",
       "120    mdrr tout le monde sais que ce mots ne veux pl...\n",
       "121    En tant que chretien, je vous le dis : vous me...\n",
       "122    Raciste et islamophobe, les mots magiques sont...\n",
       "123    Loi sur les ¬´fake news¬ª, d√©placement de la sal...\n",
       "124    @CNEWS la reaction des Israeliens pour se defe...\n",
       "125    Que toutes les personnes qui parlent de racism...\n",
       "126    @el_pedrooooo La racaille bavaroise doit √™tre ...\n",
       "127              Pas etonnant... https://t.co/xh0TnMOtOF\n",
       "128    @CristiB_ Moi j'avoue que je suis pas fan mais...\n",
       "129    @umut_bozok @YoannGillet_fn @FN_Occitanie @obj...\n",
       "130    @YoannGillet_fn @umut_bozok @FN_Occitanie @obj...\n",
       "131    Shirley; tu n‚Äôes pas maghr√©bine donc tu accept...\n",
       "Length: 132, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 14 11:56:35 2018\n",
    "\n",
    "@author: Eduard\n",
    "\n",
    "Ce script sert √† parser les messages texte des Tweets ayant √©t√© scrap√©s incorporant\n",
    "l'un de nos termes cible. Ils sont tous concatenn√©s dans un fichier CSV pour lecture\n",
    "ais√©e sur Excel et lab√©lisation manuelle.\n",
    "\n",
    "\"\"\"\n",
    "import json, glob, os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# On indique le chemin vers les fichiers JSON de Tweets scrapp√©s\n",
    "path = os.getcwd()+'/StreamListenerTweepy/Streams/*.jsonl'\n",
    "\n",
    "\n",
    "# On cr√©e deux fonctions pour it√©rer le parse sur plusieurs fichiers JSON \n",
    "# plac√©s sur le m√™me dossier\n",
    "def read_json_files(path_to_file):\n",
    "    with open(path_to_file) as p:\n",
    "        data = pd.read_json(p, lines=True)\n",
    "    return data\n",
    "\n",
    "def giant_list(json_files):\n",
    "    data_list = []\n",
    "    for f in json_files:\n",
    "        data_list.append(read_json_files(f))\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# √âxecution de fonctions et chargement de JSON dans un DF concatenn√©\n",
    "event_files = glob.glob(path)\n",
    "tweets = pd.concat(giant_list(event_files), ignore_index=True)\n",
    "tweets = tweets[[\"extended_tweet\", \"retweeted_status\"]]\n",
    "tweets_copy = tweets.copy()\n",
    "\n",
    "\n",
    "# Extraction des index des lignes non vides et √† parser\n",
    "index_ext = tweets.extended_tweet[tweets.extended_tweet.notnull()].index\n",
    "index_ret = tweets.retweeted_status[tweets.retweeted_status.notnull()].index\n",
    "\n",
    "\n",
    "# On remplace les tweets originaux et trunqu√©s par le texte en entier\n",
    "for i in index_ext:\n",
    "    tweets[\"extended_tweet\"][i] = tweets.extended_tweet[i][\"full_text\"]\n",
    "\n",
    "    \n",
    "# Pour les messages retweet√©s (reteewted_status) on a stream√© un dict √† plusiers niveaux\n",
    "# Chaque json a une structure diff√©rente. Nous voulons r√©cup√©rer deux champs:\n",
    "# 1. les messages retweet√©s en entier et 2. les messages cit√©s par le m√™me usager\n",
    "\n",
    "\n",
    "# 1. Si le stream a un champ \"extended_tweet\" on r√©cup√®re les index des ces tweets\n",
    "# pour effectuer ensuite une boucle d'extraction du texte en entier.\n",
    "# Sinon (else), le message n'a pas √©t√© trunqu√©, on utilise le champ entier \"text\"\n",
    "j=[]\n",
    "for i in index_ret:\n",
    "    if \"extended_tweet\" in tweets.retweeted_status[i]:\n",
    "        j.append(i)\n",
    "    else:\n",
    "        tweets.retweeted_status[i] = tweets.retweeted_status[i][\"text\"]\n",
    "\n",
    "for i in j:\n",
    "    tweets.retweeted_status[i] = tweets.retweeted_status[i][\"extended_tweet\"][\"full_text\"]\n",
    "\n",
    "    \n",
    "# 2. Sur la m√™me colonne d'avant on trouve le texte des messages cit√©s par l'usager.  \n",
    "# On l'a √©cras√©e lors de l'op√©ration pr√©c√©dente, on utilise une copie √† la place.\n",
    "# Pour ne pas √©craser la colonne sur 'tweets', on cr√©e une nouvelle colonne \n",
    "# ou l'on placera le texte en entier\n",
    "k=[]\n",
    "for i in index_ret:\n",
    "    if \"quoted_status\" in tweets_copy.retweeted_status[i]:\n",
    "        k.append(i)\n",
    "    else:\n",
    "        tweets[\"cite_status\"] = None\n",
    "\n",
    "try:\n",
    "    for i in k:\n",
    "        tweets.cite_status[i] = tweets_copy.retweeted_status[i][\"quoted_status\"][\"extended_tweet\"][\"full_text\"]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Enfin, on fusionne tous les tweets qui doivent √™tre labelis√©es manuellement et on efface les dupliqu√©s\n",
    "tweets_dilcrah_keywords = tweets.stack().dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# √Ä cette fin, on exporte en csv pour pouvoir faire la lecture et lab√©lisation sur Excel\n",
    "tweets_dilcrah_keywords.to_csv('TweetsToLabelCSV/tweets_dilcrah_keywords.csv', sep=',', encoding='utf-8', header = False)\n",
    "\n",
    "\n",
    "tweets_dilcrah_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
